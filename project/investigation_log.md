# Learning Time Series Volatily (CIS 696)

Can a recurrent neural network learn volatility (conditional variance) of energy prices generated by a GARCH model?

## Data

2 years of real-time and day-ahead wholesale energy prices from PJM (Aug 2002 - Aug 2004), the US mid-Atlantic RTO/ISO. This is average data for the whole market, expressed in [dollars per megawatt-hour.](https://www.pjm.com/en/Glossary#index_L)

Note: in August 2003, found in the middle of this dataset, occured the [Northeast blackout of 2003](https://en.wikipedia.org/wiki/Northeast_blackout_of_2003). This event represents an exogenous shock to the energy market.

## Framework

Chollet & Allaire's generator-based framework from their 2018 book, "Deep Learning with R".

## January

* Python/Keras approach re-written for R/Keras.
* Browser-based RStudio session tunneled through SSH on ghost employed for convenient coding/visualiation.
* Chollet & Allaire's temperature forecasting example implement for sanity-checking on new system ghost.
* Naive baseline MSE of ___ found.
* Pricing data is re-assessed in terms of log-returns, the "hourly log-percentage price change" commonly used in finance.

## 28 Jan 2019 (snow week)
### GARCH hyper-parameter search

R code was written to search for the best-fitting GARCH model among three likely sub-model types:
* sGARCH
* gjrGARCH
* csGARCH

Typical parameters were searched: GARCH(p, q) from 1 -> 2 and AR MA parameters from 0 -> 1. Guidance in this area provided by the [GARCH models with R Datacamp course.](https://www.datacamp.com/courses/garch-models-in-r)

Results:
1. Day-ahead log-return data
  * model       - gjrGARCH
  * garchOrder  - (2,2)
  * armaOrder   - (1,1)
  * distribution - sstd (skewed Student's t)
2. Real-time log-return data
  * model       - gjrGARCH
  * garchOrder  - (1,2)
  * armaOrder   - (1,1)
  * distribution - sstd (skewed Student's t)
  
## 04 Feb 2019
### Dropout and stacking

* Dropout (0.1) and recurrent dropout (0.5) added to a 1-layer 32-node LSTM model to compare to previous Python experiments.
  - Unfortunately, significant over-fitting in training still obvious.
  - Validation loss results are underwhelming.
* A stacked 2-layer (64- & 128-node) LSTM model with dropout and recurrent dropout run to compare to previous Python experiments.
  - Significant over-fitting in training still found
  - Validation loss results still poor.

## 11 Feb 2019
### Weight regularization
