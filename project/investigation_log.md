# Learning Time Series Volatily (CIS 696)

Can a recurrent neural network learn volatility (conditional variance) of energy prices generated by a GARCH model?

## Data

2 years (Aug 2002 - Aug 2004) of real-time and day-ahead wholesale energy prices from [PJM](https://www.pjm.com/markets-and-operations/energy.aspx), the US mid-Atlantic RTO/ISO. This is average data for the whole market, expressed in [dollars per megawatt-hour.](https://www.pjm.com/en/Glossary#index_L)

Note: in August 2003, found in the middle of this dataset, occured the [Northeast blackout of 2003](https://en.wikipedia.org/wiki/Northeast_blackout_of_2003). This event represents an exogenous shock to the energy market.

## Framework

Chollet & Allaire's generator-based framework from their 2018 book, "Deep Learning with R".

## January

* Python/Keras approach re-written for R/Keras.
* Browser-based RStudio session tunneled through SSH on ghost employed for convenient coding/visualiation.
* Chollet & Allaire's temperature forecasting example implement for sanity-checking on new system ghost.
* Naive baseline MSE of 0.8749 found.
* Pricing data is re-assessed in terms of log-returns, the "hourly log-percentage price change" commonly used in finance.

## 28 Jan 2019 (snow week)
### GARCH hyper-parameter search

R code was written using [rugarch](https://cran.r-project.org/web/packages/rugarch/index.html) to search for the best-fitting GARCH model among three likely sub-model types:
* sGARCH    - (Bollerslev, 1986) standard GARCH model
* gjrGARCH  - (Glosten et al., 1993) models positive and negative shocks asymmetrically (i.e. in Econ/Finance negative shocks are deeper)
* csGARCH   - (Lee & Engle, 1999) decomposes  the  conditional  variance  into  a  permanent and transitory component so as to investigate the long- and short-run movements of volatility

Typical parameters were searched: GARCH(p, q) from 1 -> 2 and AR MA parameters from 0 -> 1. Guidance in this area provided by the [GARCH models with R Datacamp course.](https://www.datacamp.com/courses/garch-models-in-r)

Criteria were minimizing the mean of information criteria(Akaike, Bayes, Shibata, Hannan-Quinn) or maximizing the log-likelihood function.

Results:
1. Day-ahead log-return data
  * model       - gjrGARCH
  * garchOrder  - (2,2)
  * armaOrder   - (1,1)
  * distribution - sstd (skewed Student's t)
2. Real-time log-return data
  * model       - gjrGARCH
  * garchOrder  - (1,2)
  * armaOrder   - (1,1)
  * distribution - sstd (skewed Student's t)
  
## 04 Feb 2019
### Dropout and stacking

* Dropout (0.1) and recurrent dropout (0.5) added to a 1-layer 32-node LSTM model to compare to previous Python experiments.
  - Unfortunately, significant over-fitting in training still obvious.
  - Validation loss results are underwhelming.
* A stacked 2-layer (64- & 128-node) LSTM model with dropout and recurrent dropout run to compare to previous Python experiments.
  - Significant over-fitting in training still found
  - Validation loss results still poor.

## 11 Feb 2019
### Weight regularization, testing subset

* Single-layer LSTM with dropout, recurrent dropout, and weight regularization.
 - 64 nodes, L1 and L2 regularization
 - to specify a GPU in R/Keras, set envvar Sys.setenv("CUDA_VISIBLE_DEVICES" = 0 or 1)

#### 2019-02-15
- 1 layer LSTM, 64 nodes
  - dropout = 0.05, recurrent_dropout = 0.1 
  - kernel_regularizer = regularizer_l1(0.001), recurrent_regularizer = regularizer_l2(0.01),
  - lookback = 240, step = 1, delay = 12, batch_size = 12
  - steps_per_epoch = 500, epochs = 50
- gw meeting update
  - find PoC data to show approach is sound
    - ozone, astro, etc.
  - tighten prediction windows
  - combine CONV + LSTM
  - test_data generator
